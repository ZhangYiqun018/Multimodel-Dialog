![](https://secure2.wostatic.cn/static/9efGiYvPZdGgpLkyrPZMx4/image.png)

Figure2：The architecture of VisualBERT. Image regions and language are combined with a Transformer to allow the self-attention to discover implicit alignments between language and [vision.It](http://vision.It) is pre-trained with am asked language modeling(Objective 1) ，and sentence-image prediction task (Objective 2) ， on caption data and then fine-tuned for different tasks .See：3.3 for more details.
图2: VisualBERT的架构。图像区域和语言与转换器相结合，以使自我注意发现语言和视觉之间的隐含对齐。它是预先训练的am ask语言建模 (目标1) 和句子-图像预测任务 (目标2)，在字幕数据上，然后针对不同的任务进行微调。有关更多详细信息，请参阅: 3.3。
